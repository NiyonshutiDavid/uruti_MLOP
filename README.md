# Uruti.Rw MLOps Pipeline ğŸš€

## ğŸ“‹ Project Overview

Uruti.Rw is a comprehensive Machine Learning Operations (MLOps) platform designed to classify startup ideas into three distinct categories: **Mentorship Needed**, **Investment Ready**, and **Needs Refinement**. The project implements an end-to-end ML pipeline with audio processing capabilities, allowing users to submit both text and audio inputs for real-time predictions.

### ğŸ¯ Problem Statement
In Rwanda's growing startup ecosystem, there's a need for an intelligent system that can automatically assess startup pitches and ideas, providing valuable insights to entrepreneurs, mentors, and investors about the readiness and potential of business concepts.

### ğŸ”§ Solution Architecture
The platform leverages multiple state-of-the-art NLP models (DistilBERT, RoBERTa, ALBERT) for text classification and OpenAI Whisper for audio transcription, deployed through a scalable FastAPI backend with an HTML/CSS/JavaScript dashboard for monitoring and management.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Mobile App    â”‚    â”‚   Web Dashboard â”‚    â”‚   Developer     â”‚
â”‚   (End Users)   â”‚â”€â”€â”€â”€â”‚   (HTML/CSS/JS) â”‚â”€â”€â”€â”€â”‚   Dashboard     â”‚
â”‚   Flutter/Reactâ”‚    â”‚   Frontend      â”‚    â”‚   (Flask)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   FastAPI       â”‚
                    â”‚   ML Server     â”‚
                    â”‚   (Port 8000)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   PostgreSQL    â”‚
                    â”‚   Database      â”‚
                    â”‚   (Aiven Cloud) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Features

### Core ML Capabilities
- **ğŸ¤ Audio Processing**: Real-time audio transcription using OpenAI Whisper, the dataset used was downloaded from Mozilla, you can access dataset here:
- **ğŸ“ Text Classification**: Multi-model ensemble (DistilBERT, RoBERTa, ALBERT, TensorFlow)
- **ğŸ”„ Model Retraining**: Automated retraining pipeline with user data
- **ğŸ“Š Performance Monitoring**: Real-time metrics and model performance tracking with Weights & Biases integration

### User Interface
- **ğŸ“± Mobile Application**: End-user focused app for pitch submissions
- **ğŸŒ Web Dashboard**: HTML/CSS/JavaScript frontend for general users
- **ğŸ‘¨â€ğŸ’» Developer Dashboard**: Flask-based monitoring and management interface for developers
- **ğŸ“ˆ Data Visualization**: Interactive charts and insights dashboard
- **ğŸ‘¥ User Management**: Role-based access control and API usage tracking

### DevOps & Monitoring
- **â˜ï¸ Cloud Deployment**: Scalable deployment with Docker containerization
- **ğŸ“Š Weights & Biases**: ML experiment tracking and model monitoring
- **ğŸ” Real-time Logging**: Comprehensive application and model logging
- **ğŸš¦ Health Monitoring**: API health checks and uptime monitoring
- **ğŸ”„ Load Testing**: Locust-based performance testing framework

---

## ğŸ› ï¸ Technology Stack

### Backend
- **FastAPI**: High-performance async API framework
- **Flask**: Developer dashboard and monitoring interface
- **PyTorch**: Deep learning framework for model inference
- **Transformers**: Hugging Face transformers for NLP models
- **OpenAI Whisper**: Speech-to-text transcription

### Frontend
- **HTML5/CSS3/JavaScript**: Web dashboard frontend
- **Mobile App**: Cross-platform mobile application for end users
- **Flask Templates**: Server-side rendering for developer dashboard

### Database & Storage
- **PostgreSQL**: Primary database (Aiven Cloud)
- **SQLAlchemy**: ORM for database operations
- **Alembic**: Database migration management

### ML/AI Stack
- **DistilBERT**: Lightweight transformer model (91% accuracy)
- **RoBERTa**: Robust transformer variant (91% accuracy)
- **ALBERT**: Parameter-efficient transformer (91% accuracy)
- **TensorFlow Model**: Custom neural network (90.6% accuracy)
- **Weights & Biases**: Experiment tracking and model monitoring

### Deployment & Monitoring
- **Docker**: Containerized deployment with Dockerfile
- **Locust**: Load testing and performance evaluation
- **Weights & Biases**: ML experiment tracking
- **CORS**: Cross-origin resource sharing

---

## ğŸ“ Project Structure

```
uruti_MLOP/
â”‚
â”œâ”€â”€ README.md                           # Project documentation
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ Dockerfile                         # Docker container configuration
â”œâ”€â”€ app.py                            # Flask developer dashboard
â”œâ”€â”€ main.py                           # FastAPI ML server
â”œâ”€â”€ models.py                         # Database models
â”œâ”€â”€ locustfile.py                     # Load testing configuration
â”‚
â”œâ”€â”€ URUTI_MODEL-CREATION_Notebook.ipynb # Model training notebook
â”œâ”€â”€ uruti_transcripts.csv             # Training dataset
â”œâ”€â”€ recorded_audio.wav                # Sample audio file
â”‚
â”œâ”€â”€ models/                           # Trained model artifacts
â”‚   â”œâ”€â”€ distilbert-base-uncased/     # DistilBERT model files
â”‚   â”œâ”€â”€ roberta-base/                # RoBERTa model files
â”‚   â”œâ”€â”€ albert-base-v2/              # ALBERT model files
â”‚   â””â”€â”€ tensorflow-model/            # TensorFlow model files
â”‚
â”œâ”€â”€ logs_distilbert-base-uncased/     # Training logs for DistilBERT
â”œâ”€â”€ logs_roberta-base/               # Training logs for RoBERTa
â”œâ”€â”€ logs_albert-base-v2/             # Training logs for ALBERT
â”‚
â”œâ”€â”€ results_distilbert-base-uncased/  # Model evaluation results
â”œâ”€â”€ results_roberta-base/           # Model evaluation results
â”œâ”€â”€ results_albert-base-v2/         # Model evaluation results
â”‚
â”œâ”€â”€ wandb/                          # Weights & Biases experiment logs
â”‚
â”œâ”€â”€ templates/                      # Flask HTML templates
â”‚   â””â”€â”€ ml_dashboard.html          # Developer dashboard template
â”‚
â”œâ”€â”€ static/                         # Static web assets
â”‚   â”œâ”€â”€ css/                       # Stylesheets
â”‚   â”œâ”€â”€ js/                        # JavaScript files
â”‚   â””â”€â”€ images/                    # Image assets
â”‚
â”œâ”€â”€ uruti_app/                     # Mobile application code
â”‚
â”œâ”€â”€ uruti-web-app/                 # Web frontend application
â”‚
â”œâ”€â”€ migrations/                    # Database migration files
â”‚
â””â”€â”€ __pycache__/                   # Python cache files
```

---

## ğŸ”§ Installation & Setup

### Prerequisites
- Python 3.8+
- PostgreSQL database
- Docker (optional)
- Git
- Node.js (for frontend development)

### 1. Clone the Repository
```bash
git clone https://github.com/NiyonshutiDavid/uruti_MLOP.git
cd uruti_MLOP
```

### 2. Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Environment Configuration
Create a `.env` file in the root directory:
```env
DATABASE_URL=postgresql://username:password@localhost:5432/uruti_db
SECRET_KEY=your_secret_key_here
DEBUG=True
WANDB_API_KEY=your_wandb_api_key
MODEL_PATH=./models/
WHISPER_MODEL=base
```

### 5. Database Setup
```bash
# Initialize database tables
python -c "
import asyncio
from main import init_database
asyncio.run(init_database())
"

# Run migrations
flask db upgrade
```

### 6. Download Pre-trained Models
```bash
# Models will be automatically loaded from the ./models/ directory
# Ensure all model directories are present:
# - distilbert-base-uncased/
# - roberta-base/
# - albert-base-v2/
# - tensorflow-model/
```

---

## ğŸš€ Running the Application

### Development Mode

#### Start FastAPI ML Server
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

#### Start Flask Developer Dashboard
```bash
python app.py
```

#### Serve Web Frontend
```bash
cd uruti-web-app
# Serve static files or use a local server
python -m http.server 3000
```

The services will be available at:
- **ML API**: http://localhost:8000
- **Developer Dashboard**: http://localhost:5000
- **Web Frontend**: http://localhost:3000
- **API Documentation**: http://localhost:8000/docs

### Production Deployment

#### Using Docker
```bash
# Build the image
docker build -t uruti-mlops .

# Run the container
docker run -p 8000:8000 -p 5000:5000 uruti-mlops
```

---

## ğŸ“± Demo Screenshots

### Mobile Application (End Users)
[INSERT MOBILE APP SCREENSHOTS HERE]

*Screenshot 1: Mobile app home screen with audio recording interface for startup pitch submission*

*Screenshot 2: Text input screen where users can type their startup ideas*

*Screenshot 3: Real-time prediction results showing classification and confidence scores*

*Screenshot 4: User history showing past pitch submissions and feedback*

### Web Dashboard (General Users)
[INSERT WEB FRONTEND SCREENSHOTS HERE]

*Screenshot 1: Web dashboard landing page with pitch submission interface*

*Screenshot 2: Real-time prediction results display with category breakdown*

*Screenshot 3: User analytics and submission history dashboard*

*Screenshot 4: Audio upload and transcription interface*

### Developer Dashboard (Flask)
[INSERT DEVELOPER DASHBOARD SCREENSHOTS HERE]

*Screenshot 1: Main developer dashboard with system metrics and model performance*

*Screenshot 2: Model comparison view showing all four models' performance*

*Screenshot 3: User management interface with API usage tracking*

*Screenshot 4: Model retraining interface with Weights & Biases integration*

*Screenshot 5: Real-time monitoring dashboard with system health indicators*

*Screenshot 6: Database management and migration interface*

---

## ğŸ”„ ML Pipeline Features

### 1. Data Acquisition & Upload
- **CSV Data Upload**: Support for structured data via `uruti_transcripts.csv`
- **Audio File Processing**: Real-time audio transcription and storage
- **Real-time Data Ingestion**: API endpoints for continuous data collection
- **Data Validation**: Automated quality checks and format validation
- **Storage Management**: Organized data storage with model-specific logging

### 2. Data Preprocessing
- **Text Cleaning**: Advanced NLP preprocessing for startup pitch text
- **Audio Processing**: Whisper-based transcription with noise reduction
- **Feature Engineering**: Token-level feature extraction for transformer models
- **Data Augmentation**: Synthetic data generation for model robustness

### 3. Model Training & Evaluation
- **Multi-Model Ensemble**: Training of 4 different model architectures
- **Transfer Learning**: Fine-tuning pre-trained transformer models
- **Weights & Biases Integration**: Comprehensive experiment tracking
- **Cross-validation**: Robust evaluation across model variants
- **Model Versioning**: Systematic tracking of model iterations

### 4. Model Deployment & Serving
- **Real-time Inference**: Low-latency prediction API with model ensemble
- **Model Selection**: Dynamic selection based on performance metrics
- **Batch Processing**: Bulk prediction capabilities
- **A/B Testing**: Comparison across different model architectures

### 5. Monitoring & Maintenance
- **Weights & Biases Monitoring**: Real-time performance tracking
- **Usage Analytics**: API call patterns and user behavior analysis
- **Model Drift Detection**: Performance degradation alerts
- **Automated Retraining**: Triggered retraining based on performance metrics

---

## ğŸ“Š Model Performance & Evaluation

### Current Model Metrics

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| **DistilBERT-base-uncased** | **91.0%** | **82.0%** | **91.0%** | **86.0%** |
| **RoBERTa-base** | **91.0%** | **82.0%** | **91.0%** | **86.0%** |
| **ALBERT-base-v2** | **91.0%** | **82.0%** | **91.0%** | **86.0%** |
| **TensorFlow Model** | **90.6%** | **82.0%** | **91.0%** | **86.0%** |

### Evaluation Methodology
The model evaluation process includes:

1. **Multi-Model Training**: Four different architectures trained on the same dataset
2. **Comprehensive Metrics**: Accuracy, Precision, Recall, and F1-Score for each model
3. **Weights & Biases Tracking**: All experiments logged and tracked
4. **Cross-Validation**: Consistent evaluation methodology across models
5. **Performance Comparison**: Side-by-side analysis of model architectures

### Model Selection Strategy
- **Ensemble Approach**: Combining predictions from top-performing models
- **Latency Optimization**: DistilBERT selected for production due to speed
- **Accuracy Consistency**: All transformer models achieving 91% accuracy
- **Resource Efficiency**: ALBERT provides parameter efficiency

### Weights & Biases Integration
- **Experiment Tracking**: All training runs logged with hyperparameters
- **Model Comparison**: Visual comparison of model performance
- **Loss Visualization**: Training and validation loss curves
- **Hyperparameter Optimization**: Systematic parameter tuning

---

## ğŸ”„ Retraining Pipeline

### Automated Retraining Triggers
1. **Performance Degradation**: Accuracy drops below 89%
2. **Data Drift Detection**: Statistical changes in input distribution
3. **Schedule-based**: Weekly retraining cycles
4. **Manual Trigger**: On-demand retraining via developer dashboard

### Retraining Process
1. **Data Collection**: Aggregate new labeled data from user feedback
2. **Data Preprocessing**: Clean and prepare new training data
3. **Multi-Model Training**: Retrain all four model architectures
4. **Validation**: Evaluate models against holdout test set
5. **Model Selection**: Choose best performing model for deployment
6. **Weights & Biases Logging**: Track all retraining experiments

### Retraining Monitoring
- **Training Progress**: Real-time training metrics via Weights & Biases
- **Resource Usage**: GPU/CPU utilization during training
- **Model Comparison**: Performance analysis across architectures
- **Experiment History**: Complete training run history and artifacts

---

## ğŸ” API Documentation

### Authentication
```bash
# Most endpoints support optional user identification
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"text": "Your startup pitch here", "user_id": "user@example.com"}'
```

### Core Endpoints

#### Text Prediction
```bash
POST /predict
{
  "text": "We are building an AI-powered solution for small businesses in Rwanda",
  "user_id": "optional_user_id"
}
```

#### Audio Prediction
```bash
POST /predict
Content-Type: multipart/form-data
file: [audio_file.wav]
user_id: optional_user_id
```

#### Model Metrics
```bash
GET /metrics
# Returns performance metrics for all four models
```

#### Training Status
```bash
GET /training/status
POST /training/start
{
  "model_type": "distilbert",  # or "roberta", "albert", "tensorflow"
  "batch_size": 32,
  "learning_rate": 0.001,
  "epochs": 10
}
```

---

## ğŸ§ª Testing & Load Performance

### Load Testing with Locust

#### Running Load Tests
```bash
# Start the locust web interface
locust -f locustfile.py --host=http://localhost:8000

# Or run headless mode
locust -f locustfile.py --host=http://localhost:8000 --users 100 --spawn-rate 10 --run-time 300s --headless
```

#### Test Configuration
- **Target URL**: http://localhost:8000
- **Test Duration**: 10 minutes
- **Concurrent Users**: 1, 10, 50, 100, 500
- **Request Types**: Text and Audio predictions

#### Performance Results

##### Single Container Performance
| Users | RPS | Avg Response Time | 95th Percentile | Failure Rate |
|-------|-----|-------------------|-----------------|--------------|
| 1     | 22  | 45ms             | 89ms           | 0%          |
| 10    | 180 | 55ms             | 120ms          | 0%          |
| 50    | 650 | 77ms             | 180ms          | 0.1%        |
| 100   | 850 | 118ms            | 290ms          | 2.3%        |
| 500   | 920 | 540ms            | 1200ms         | 15.7%       |

##### Model-Specific Performance
- **DistilBERT**: Fastest inference (35ms avg)
- **RoBERTa**: Moderate speed (55ms avg)
- **ALBERT**: Parameter efficient (45ms avg)
- **TensorFlow**: Custom model (40ms avg)

---

## ğŸ“ˆ Data Insights & Visualizations

### Dataset Analysis (`uruti_transcripts.csv`)

#### Feature Distribution Analysis
1. **Text Length Distribution**: Startup pitches range from 20-500 words
2. **Category Balance**: 
   - Mentorship Needed: 35%
   - Investment Ready: 30%
   - Needs Refinement: 35%
3. **Audio Quality Patterns**: Higher quality audio correlates with better transcription

#### Model Performance Insights
1. **Transformer Consistency**: All transformer models achieve identical 91% accuracy
2. **TensorFlow Performance**: Custom model slightly lower at 90.6%
3. **Training Convergence**: Models converge within 5-10 epochs
4. **Feature Importance**: Business model clarity most predictive

#### Weights & Biases Insights
1. **Learning Curves**: Consistent learning patterns across models
2. **Hyperparameter Impact**: Learning rate most critical parameter
3. **Overfitting Detection**: Early stopping prevents overfitting
4. **Resource Usage**: DistilBERT most efficient for production

---

## ğŸ” Security & Privacy

### Data Protection
- **Encryption**: All data encrypted in transit and at rest
- **Access Control**: Role-based permissions (End Users vs Developers)
- **Privacy**: User data anonymization and GDPR compliance
- **Audit Logging**: Comprehensive access and modification logs

### Security Measures
- **Input Validation**: Strict validation of all user inputs
- **Rate Limiting**: API call limits to prevent abuse
- **SQL Injection Prevention**: Parameterized queries and ORM usage
- **CORS Configuration**: Restricted cross-origin requests

---

## ğŸ¤ Contributing

### Development Workflow
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Standards
- **Python Style**: Follow PEP 8 guidelines
- **Documentation**: Comprehensive docstrings for all functions
- **Testing**: Write tests for new features and bug fixes
- **Weights & Biases**: Log all experiments and model changes

---

## ğŸ“ Support & Contact

### Project Team
- **Lead Developer**: David Niyonshuti (david@uruti.rw)
- **ML Engineer**: [Team Member] (email@uruti.rw)
- **Frontend Developer**: [Team Member] (email@uruti.rw)

### Getting Help
- **Issues**: Report bugs via GitHub Issues
- **Documentation**: Check our comprehensive wiki
- **Community**: Join our Slack workspace
- **Email**: Contact support@uruti.rw

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Hugging Face**: For the incredible Transformers library and pre-trained models
- **OpenAI**: For the Whisper speech recognition model
- **Weights & Biases**: For comprehensive ML experiment tracking
- **FastAPI Team**: For the high-performance web framework
- **Aiven**: For reliable cloud database services
- **Rwanda Innovation Hub**: For supporting local tech innovation

---

## ğŸ¬ Demo Video

**YouTube Demo**: [INSERT YOUTUBE LINK HERE]

The demo video showcases:
- Complete end-to-end user journey on mobile app
- Web dashboard functionality for general users
- Developer dashboard with model monitoring
- Real-time prediction and audio processing
- Multi-model comparison and performance metrics
- Model retraining with Weights & Biases integration
- Load testing results and scalability demonstration

---

*Last updated: [Current Date]*
*Version: 2.1.0*
*Models: DistilBERT, RoBERTa, ALBERT, TensorFlow (91% accuracy)*